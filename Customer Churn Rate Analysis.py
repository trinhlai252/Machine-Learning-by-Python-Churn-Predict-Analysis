# -*- coding: utf-8 -*-
"""ML_lastprj_LaiThucTrinh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s2LUfySwxbeb1RhCHwal-YVFFQUzZTXG

##Load data
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import numpy as np
import warnings
import time
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/Data_ML_Project/'

df= pd.read_excel(path+"churn_prediction.xlsx")
df.head(10)

"""#EDA

##Overview
"""

df.describe()

df.info()

"""đổi tên các cột thành chữ thường

"""

# Đổi tên tất cả các cột trong df thành chữ thường
df.columns = df.columns.str.lower()

df.info()

"""##Check null, check dup"""

null_check =pd.DataFrame({
    'null_count': df.isnull().sum(),
    'null_percentage': df.isnull().sum() * 100/len(df)
})
print(null_check)

col_null = null_check[null_check['null_percentage']>0].index.tolist()
col_null

for col in col_null:
    plt.figure(figsize=(10, 6))  # Set the size of the plot
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot of {col}')  # Title of the plot
    plt.show()

"""nhật xét: các cột trên đều tồn tại outliers => replace null = median"""

for col in col_null:
    median_value = df[col].median()  # calculate median
    df[col].fillna(median_value, inplace=True)  #replace

print(df[col_null].isnull().sum())

"""check dup"""

df.duplicated().sum()*100/len(df)

"""check imbalanced"""

label_ratio = df['churn'].value_counts(normalize=True)
label_ratio

"""##Check features"""

def calculate_churn_ratio(df, column):

    count_customer = df.groupby(column).agg(
        total_cus=('customerid', 'count'),  # tổng cus
        churn_cus=('churn', 'sum')        # số churn
    ).reset_index()


    count_customer['churn_ratio'] = count_customer['churn_cus'] / count_customer['total_cus'] * 100

    count_customer = count_customer.sort_values(by='churn_ratio', ascending=False)

    return count_customer

"""###Check density of some features: WarehouseToHome, HourSpendOnApp,	CashbackAmount"""

cols = ['warehousetohome','hourspendonapp', 'cashbackamount']
num_cols = df[cols]

plt.figure(figsize=(10, 6))

for i, col in enumerate(cols, 1):
    plt.subplot(len(cols), 1, i)
    sns.kdeplot(num_cols[col], shade=True)
    plt.title(f'Density Distribution of {col}')
    plt.xlabel('Value')
    plt.ylabel('Density')

plt.tight_layout()
plt.show()

cols = ['warehousetohome', 'hourspendonapp', 'cashbackamount']
plt.figure(figsize=(8, 12))

for i, col in enumerate(cols, 1):
    plt.subplot(len(cols), 1, i)
    sns.boxplot(data=df, x='churn', y=col)

    plt.title(f'Distribution of {col} between Churn and Non-Churn')
    plt.xlabel('Churn')
    plt.ylabel(col)

plt.tight_layout()
plt.show()

"""=>  maybe 3 featues are related to churn

###Check tenure
"""

tenure_churnrate = calculate_churn_ratio(df, 'tenure')
tenure_churnrate.sort_values(by='churn_ratio', ascending=False).head(5)

plt.figure(figsize=(8, 6))
sns.boxplot(data=df, x='churn', y='tenure')
plt.title('Distribution of Tenure between Churn and Non-Churn')
plt.xlabel('Churn')
plt.ylabel('Tenure')
plt.show()

"""=>tenure is related

###Check Citytier
"""

citytier_churnrate = calculate_churn_ratio(df, 'citytier')
citytier_churnrate.sort_values(by='churn_ratio', ascending=False)

plt.figure(figsize=(8, 6))
city_df = calculate_churn_ratio(df, 'citytier')
sns.boxplot(data = city_df, y='churn_ratio')
plt.title(f'Churn ratio Distribution of citytier')
plt.xlabel('Value')
plt.ylabel('Churn Ratio')

"""=> related

###Check Numberofdevice
"""

numberdevide_churnrate = calculate_churn_ratio(df, 'numberofdeviceregistered')
numberdevide_churnrate.sort_values(by='churn_ratio', ascending=False)

"""=> related

###Check satisfactionscore
"""

satisfaction_churnrate = calculate_churn_ratio(df, 'satisfactionscore')
satisfaction_churnrate.sort_values(by='churn_ratio', ascending=False)

"""=>related

###Check numberofaddress
"""

numberaddress_churnrate = calculate_churn_ratio(df, 'numberofaddress')
numberaddress_churnrate.sort_values(by='churn_ratio', ascending=False)

plt.figure(figsize=(8, 6))  # Set the size of the plot
sns.boxplot(x=df['numberofaddress'])
plt.title(f'Boxplot of numberofaddress')  # Title of the plot
plt.show()

"""==>maybe realted

###Check Complain
"""

complain_churnrate = calculate_churn_ratio(df, 'complain')
complain_churnrate.sort_values(by='churn_ratio', ascending=False)

"""=>related

###Check orderamounthike, couponused, ordercount, daysincelastorder
"""

orderamounthike_churnrate = calculate_churn_ratio(df, 'orderamounthikefromlastyear')
orderamounthike_churnrate.sort_values(by='churn_ratio', ascending=False)

couponuse_churnrate = calculate_churn_ratio(df, 'couponused')
couponuse_churnrate.sort_values(by='churn_ratio', ascending=False)

daysincelastorder_churnrate = calculate_churn_ratio(df, 'daysincelastorder')
daysincelastorder_churnrate.sort_values(by='churn_ratio', ascending=False)

ordercount_churnrate = calculate_churn_ratio(df, 'ordercount')
ordercount_churnrate.sort_values(by='churn_ratio', ascending=False)

cols = ['orderamounthikefromlastyear', 'couponused', 'ordercount', 'daysincelastorder']
num_cols = df[cols]

plt.figure(figsize=(10, 8))

for i, col in enumerate(cols, 1):
    plt.subplot(len(cols), 1, i)
    sns.boxplot(data=df, x='churn', y=col)

    plt.title(f'Distribution of {col} between Churn and Non-Churn')
    plt.xlabel('Churn')
    plt.ylabel(col)

plt.tight_layout()
plt.show()

"""=> couponused, ordercount are related

###Check gender
"""

gender_churnrate= calculate_churn_ratio(df, 'gender')
gender_churnrate

"""=>gender not ralated

###check prefeturedlogindevide
"""

devide_churnrate = calculate_churn_ratio(df, 'preferredlogindevice')
devide_churnrate

"""=> devide no related

###Check PreferredPaymentMode
"""

payment_churnrate = calculate_churn_ratio(df, 'preferredpaymentmode')
payment_churnrate.sort_values(by='churn_ratio', ascending=False)

payment_churnrate.sort_values(by='churn_cus', ascending=False)

"""=> maybe paymen not ralated

###Check PreferedOrderCat
"""

Ordercat_churnrate = calculate_churn_ratio(df, 'preferedordercat')
Ordercat_churnrate.sort_values(by='churn_ratio', ascending=False)

"""=>maybe preferedorderca is related to churn rate

###Check maritalstatus
"""

marital_churnrate = calculate_churn_ratio(df, 'maritalstatus')
marital_churnrate.sort_values(by='churn_ratio', ascending=False)

"""=> maritalstatus is ralated to churn

#Feature Transforming
"""

list_columns_to_drop = ['customerid','gender','maritalstatus','preferredpaymentmode','preferredlogindevice','orderamounthikefromlastyear']
df_drop = df.drop(columns = list_columns_to_drop)
cate_columns = df_drop.loc[:, df_drop.dtypes == object].columns.tolist()

encoded_df = pd.get_dummies(df_drop, columns = cate_columns,drop_first=True)
encoded_df.head(3)

"""#Model Traning

##Split train/validate/test set
"""

from sklearn.model_selection import train_test_split
x=encoded_df.drop('churn', axis = 1)
y=encoded_df[['churn']]


x_train, x_temp, y_train, y_temp = train_test_split(x,y, test_size=0.3, random_state=42)

x_val, x_test, y_val, y_test = train_test_split(x_temp,y_temp, test_size=0.5, random_state=42)

print(f"Number data of train set: {len(x_train)}")
print(f"Number data of validate set: {len(x_val)}")
print(f"Number data of test set: {len(x_test)}")

"""## Normalization for each set"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

scaler.fit(x_train)

x_train_scaled = scaler.transform(x_train)
x_val_scaled = scaler.transform(x_val)
x_test_scaled = scaler.transform(x_test)

"""## Apply Model

###Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

clf_logis = LogisticRegression(random_state = 0)
clf_logis.fit(x_train_scaled, y_train) #model to learn

y_pred_val = clf_logis.predict(x_val_scaled) #model to predict on val set
y_pred_train = clf_logis.predict(x_train_scaled) #Predict back on train to check overfit

"""###Random Forest"""

from sklearn.ensemble import RandomForestClassifier

clf_rand = RandomForestClassifier(max_depth=15, random_state=0, n_estimators = 100)

clf_rand.fit(x_train_scaled, y_train)

y_ranf_pre_train = clf_rand.predict(x_train_scaled) #Predict back on train to check overfit
y_ranf_pre_val = clf_rand.predict(x_val_scaled) #Model to predict on val set

"""#Model Evaluation"""

from sklearn.metrics import balanced_accuracy_score, f1_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

"""##Logistic Regression"""

balanced_accuracy_train = balanced_accuracy_score(y_train, y_pred_train)
balanced_accuracy_val = balanced_accuracy_score(y_val, y_pred_val)
print(balanced_accuracy_train,balanced_accuracy_val)

cm = confusion_matrix(y_val, y_pred_val, labels=clf_logis.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_logis.classes_)
disp.plot()

"""##Random Forest"""

balanced_accuracy_train = balanced_accuracy_score(y_train, y_ranf_pre_train)
balanced_accuracy_val = balanced_accuracy_score(y_val, y_ranf_pre_val)
print(balanced_accuracy_train,balanced_accuracy_val)

"""=> sử dụng random forest

##Hyperparameter Tuning
"""

from sklearn.model_selection import GridSearchCV
# Xác định các params để GridSearch chạy qua thử (nếu code chạy lâu quá, có thể giảm bớt số lượng params lại)
param_grid = {
    'n_estimators': [10, 50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Use GridSearchCV to find the best parameters
# Ở đây mình dùng scoring là balanced_accuracy, mọi người có thể tùy chỉnh tùy vào mục đích model của mn nhé
grid_search = GridSearchCV(clf_rand, param_grid, cv=5, scoring='balanced_accuracy')

# Fit the model
grid_search.fit(x_train, y_train)

# Print the best parameters
print("Best Parameters: ", grid_search.best_params_)

# Evaluate the best model on the test set
best_clf = grid_search.best_estimator_
accuracy = best_clf.score(x_test, y_test)
print("Test set accuracy: ", accuracy)

best_params = grid_search.best_params_
clf_rand_after = RandomForestClassifier(**best_params,random_state=0)

clf_rand_after.fit(x_train, y_train)
y_ranf_aft_train = clf_rand_after.predict(x_train)
y_ranf_aft_val = clf_rand_after.predict(x_val)

print(f'Balance accuracy of train set: {balanced_accuracy_score(y_train, y_ranf_aft_train)}')
print(f'Balance accuracy of val set: {balanced_accuracy_score(y_val, y_ranf_aft_val)}')

"""#Cluster churn customers


"""

df_churn = df[df['churn']==1]
df_churn.drop(columns = ['customerid','churn'],inplace=True)
df_churn.head()

#Transform data:
encoded_df = pd.get_dummies(df_churn, columns = cate_columns,drop_first=True)
encoded_df.head(3)

encoded_df = encoded_df.select_dtypes(include=[float, int])

#Normalization:
scaler = MinMaxScaler()
model=scaler.fit(encoded_df)
scaled_data=model.transform(encoded_df)
scaled_df = pd.DataFrame(scaled_data, columns = encoded_df.columns.tolist())

scaled_df.describe()

scaled_df.info()

"""Dimension Reduction"""

from sklearn.decomposition import PCA
pca=PCA(n_components=3)
pca.fit(scaled_df)
PCA_df=pd.DataFrame(pca.transform(scaled_df), columns=['PC1', 'PC2', 'PC3'])
PCA_df.head()

pca.explained_variance_ratio_

"""Choosing K"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt, numpy as np
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import AgglomerativeClustering
from matplotlib.colors import ListedColormap

ss = []
max_clusters = 10
for i in range(1, max_clusters+1):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(scaled_df)
    # Inertia method returns WCSS for that model
    ss.append(kmeans.inertia_)

# Plot the Elbow method
plt.figure(figsize=(10,5))
plt.plot(range(1, max_clusters+1), ss, marker='o', linestyle='--')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

"""=> We can not cluster churn customers"""